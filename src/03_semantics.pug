doctype html
html(lang="en")
  head
    meta(charset="UTF-8")
    meta(name="viewport", content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no")
    title Fundamentals of Linguistic Data Science
    link(rel="stylesheet" href="dist/reset.css")
    link(rel="stylesheet" href="dist/reveal.css")
    link(rel="stylesheet" href="dist/theme/beige.css")
    link(rel="stylesheet" href="plugin/highlight/monokai.css")
    div(class="reveal")
      div(class="slides")
        section(data-background-image="img/tree.png" data-background-opacity=0.5)
          h1 Semantics
          h3 John P. McCrae - University of Galway
          h5 Course at ESSLLI 2023
        section(data-background-image="https://i.makeagif.com/media/6-07-2022/0QaCig.gif" data-background-opacity=0.5)
          h1 Vector space models
        section
          h2 Vector space models
          p Mathematical model for representing text as a vector of numbers. 
          p Enables linear algebra to analyse text.
        section
          h2 Term-document matrix
          p A term-document matrix is a matrix where each row represents a term and each column represents a document.
          table 
            tr
              td
              td Document 1
              td Document 2
              td Document 3
            tr
              td Term 1
              td 1
              td 0
              td 1
            tr
              td Term 2
              td 0
              td 1
              td 1
            tr
              td Term 3
              td 1
              td 1
              td 0
        section
          h2 Term-document matrix
          p Python example
          pre
            code(lang="python").
              term_document_matrix = np.zeros((len(vocabulary), len(documents)))
              for doc in documents:
                  for term in doc:
                    term_document_matrix[term][doc] += 1
        section
          h2 Similarity measures
          p Cosine similarity (angle between vectors)
          p Euclidean distance (distance between vectors)
          pre
            code(lang="python").
              from sklearn.metrics.pairwise import cosine_similarity, euclidean_distance
              cosine_similarity(term_document_matrix[0], term_document_matrix[1])
              euclidean_distance(term_document_matrix[0], term_document_matrix[1])
        section
          h2 Term-document matrix
          p Very large
          p Very sparse (many zeros)
          p Not very informative
        section
          h1 Word embeddings
        section
          h2 Word embeddings
          p Instead of a large vector compress all this information into a small vector.
          img(src="img/embedding_compress.svg")
        section
          h2 Word embeddings - Autoencoders
          img(src="img/autoencoder.svg")
        section
          h2 Word embeddings - Word2Vec
          img(src="img/word2vec.svg")
        section
          h2 Analogy
          p Word vectors capture linguistic regularities
          
          p vec(Berlin) â‰ƒ vec(Germany) + vec(Paris) - vec(France)
          img(src="img/analogy.svg" width="50%")
        section
          h1 Understanding semantic spaces
        section
          h2 Loading word embeddigns
        section
          h2 
        section
          h1 Pretrained language models
        section
          h2 Language Models
          p A language model is a function that calculates the likelihood of a string of words.
          p P("this string") = 0.0001
        section
          h2 What's the big deal???
          p The probability of text is higher if the text is:
          ul 
            li In a language
            li Grammatically order
            li Coherent
            li Plausible
        section
          h2 Generative Language Models
          p Languages models can generate text

          p \[ \max_{w \in \mathrm{vocabulary}} p(\mathrm{what~is~the~next~} w)\]
          p Repeating this allows us to generate text
        section
          h2 Pretraining
          p Most models are trained <em>autoregressively</em>
          p Can you guess the word?
          ul
            li(class="fragment") for the humanities,  literature and culture in the ????
            li(class="fragment") the theme of the album is the life of the ??? 
            li(class="fragment") shipping in the caribbean and off the ???
            li(class="fragment") she was the daughter of an african ???
        section
          h2 Transformers
          p Most popular architecture at the moment
          p Why transformers?
          p Natural language is hard to do math with
          ul
            li Words not numbers
            li Sentences of different lengths
        section
          h2 Transformers
          img(src="img/transformers.svg", width="50%")
        section
          h2 ChatGPT
          p(style="font-size:1.5em") <b>G</b>enerative <b>P</b>re-Trained <b>T</b>ransfomer
        section
          h1 Prompt Engineering
        section
          h2 Prompt Engineering
          p Large (>10B parameter) models demonstrate emergent properties
          p Using the correct initial text (prompt) we can extra information from the language model
          a(href="https://huggingface.co/google/flan-t5-base") Huggingface Hub
        section
          h2 Zero-shot prompting
          p Input:
          pre
            | Classify the text into neutral, negative or positive. 
            | Text: I think the vacation is okay.
            | Sentiment: 
          p Output:
          pre
            | positive
        section
          h2 Few-shot prompting
          p Input:
          pre 
            | workable: work
            | edible: eat
            | visible: see
            | readble:
          p Ouput:
          pre
            | read
        section
          h2 Chain-of-thought prompting
          p Input:
          pre
            | Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
            | A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.
            | The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. 
          p Output:
          pre
            Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.
        section
          h2 Self-evaluation
          p Input:
          pre
            What is 9 + 10?
          p Output:
          pre
            | 21
          p Input:
          pre
            | What is 9 + 10?
            | 21
            | Do you think 21 is the correct answer?
          p Output:
          pre
            | No
    script(src="dist/reveal.js")
    script(src="plugin/notes/notes.js")
    script(src="plugin/markdown/markdown.js")
    script(src="plugin/highlight/highlight.js")
    script(src="plugin/math/math.js")
    script.
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
              hash: true,
              slideNumber: true,
    
              // Learn about plugins: https://revealjs.com/plugins/
              plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
  
